# SM-CPTD — Workflow Scheduling with Stable Matching and Task Duplication

## Overview
This project implements a workflow scheduling pipeline inspired by the SM-CPTD approach:

1. **DCP** computes task ranks and selects a (level-wise) critical path.
2. **SMGT** assigns tasks to VMs level-by-level using a stable matching strategy, giving priority to critical-path tasks.
3. **LOTD** improves the schedule by duplicating tasks (in this code: mainly entry tasks) when that reduces communication overhead.

The repository also contains a full experimental runner and plotting scripts to reproduce the produced figures.

Reference: “A stable matching-based algorithm for task scheduling in cloud computing” (Journal of Supercomputing, 2021).

Main objectives:
- Implement the scheduling pipeline (DCP → SMGT → LOTD) on real Pegasus workflow graphs.
- Run controlled experiments sweeping CCR values and VM counts.
- Save results in machine-readable format and generate publication-style figures.

### Project Structure

```
stable-matching-game-theory/
├── algorithms/                 Java implementation and experiment runners
│   ├── Main.java               Entry point (runs ExperimentRunner and optional plotting)
│   ├── ExperimentRunner.java   Runs the experiments and saves results
│   ├── SMCPTD.java             Orchestrates DCP → SMGT → LOTD
│   ├── DCP.java                Critical path selection via recursive ranking
│   ├── SMGT.java               Stable matching-based scheduling per DAG level
│   ├── LOTD.java               Task duplication and schedule timing recomputation
│   ├── PegasusXMLParser.java   Converts Pegasus XML workflows to CSV datasets
│   ├── DataLoader.java         Loads DAG structure and generates numeric parameters
│   ├── Metrics.java            ET/SLR/AVU/VF + communication cost helpers
│   ├── CCRAnalyzer.java        Writes CCR sensitivity snapshots to JSON
│   └── GanttChartGenerator.java Writes optional Gantt JSON snapshots
├── generators/
│   ├── generate_paper_figures.py
│   ├── analyze_ccr_sensitivity.py
│   └── visualize_dag.py
├── workflow/                   Pegasus XML workflows (input)
├── results/
│   ├── experiments_results.json
│   └── figures/                Generated figures (PNG)
├── run.sh
└── clean.sh
```

### Requirements

- Java 8+ (JDK)
- Python 3 (only for figure generation)
- Python packages: `pandas`, `matplotlib`

Install Python dependencies:

```bash
pip3 install pandas matplotlib
```

### Running Experiments

Recommended (repo root):

```bash
./run.sh
```

What `run.sh` does:
1) runs `PegasusXMLParser` (generates CSV datasets under `data/`)
2) compiles and runs `Main`
3) `Main` runs `ExperimentRunner` and then tries to generate figures automatically

Manual run (no scripts):

```bash
cd algorithms
javac *.java

java Main
```

On Windows: run `run.sh` via WSL or Git Bash; otherwise use the manual Java commands.

#### Arguments

The Java runners accept a small set of CLI flags:

1. **Experiment selection**:
   - `--exp1`: run the CCR sweep only
   - `--exp2`: run the VM-count sweep only

2. **Workflow selection**:
   - `--workflow=<name>`: restrict to one workflow (`cybershake`, `epigenomics`, `ligo`, `montage`)

3. **Reproducibility**:
   - `--seed=<long>` or `--seed <long>`: set the base seed (used by `SeededRandom`)

Examples:

```bash
cd algorithms

# CCR sweep only
java ExperimentRunner --exp1

# VM-count sweep only
java ExperimentRunner --exp2

# Only Montage
java ExperimentRunner --exp1 --workflow=montage

# Fixed seed for reproducibility
java Main --seed=123
```

## Experiments Implemented (in code)

All experiments are implemented in `algorithms/ExperimentRunner.java`.

### Experiment 1 — CCR sweep

- Workflows: `cybershake`, `epigenomics`, `ligo`, `montage`
- CCR values: 0.4 → 2.0 (step 0.2)
- Sizes:
  - Small: 5 VMs and ~50 tasks (Epigenomics uses 47)
  - Medium: 10 VMs and 100 tasks
  - Large: 50 VMs and ~1000 tasks (Epigenomics uses 997)

This experiment also writes CCR sensitivity JSON snapshots via `CCRAnalyzer` under `results/ccr_sensitivity/`.

### Experiment 2 — VM-count sweep

- Fixed CCR: 1.0
- VM counts: 30, 35, 40, 45, 50, 55, 60, 65, 70
- Tasks: ~1000 (Epigenomics uses 997)

## Outputs

### Results

`ExperimentRunner` writes results under `results/`:
- `results/experiments_results.json`
- `results/experiments_results.csv` (created by the runner when saving CSV)

Recorded metrics (fields in `experiments_results.json`):
- `slr`, `avu`, `vf`, `avgSatisfaction`, `makespan`

### Figures

Figures are saved under `results/figures/`.

They are generated by `generators/generate_paper_figures.py` using `results/experiments_results.json`.
The script can be run automatically from `Main` (if Python + dependencies are available) or manually:

```bash
cd generators
python3 generate_paper_figures.py --auto
```

If the corresponding experiment data is present, the script generates (among others) the following PNG files:
- `figure3_slr_vs_ccr_small.png`
- `figure4_slr_vs_ccr_medium.png`
- `figure5_slr_vs_ccr_large.png`
- `figure6_avu_vs_ccr_small.png`
- `figure7_avu_vs_ccr_medium.png`
- `figure8_avu_vs_ccr_large.png`
- `figure9_slr_vs_vms.png`
- `figure10_avu_vs_vms.png`
- `figure_vf_vs_vms.png`
- `figure_vf_vs_workflow_1000x50_ccr1.png`
- `figure_avg_satisfaction_vs_workflow_1000x50_ccr1.png`
- `figure_metrics_comparison_large.png`

Optional outputs (enabled in Java code paths that request them):
- `results/gantt_charts/*.json` (Gantt chart snapshots)

## Data Generation and Assumptions (important for interpretation)

- The workflow structure comes from Pegasus XML files under `workflow/`.
- CSV datasets under `data/` are generated from XML by `PegasusXMLParser`.
- During execution, `DataLoader` uses the CSVs mainly for DAG structure and IDs; it generates numeric parameters uniformly:
  - task size in [500, 700]
  - VM capacity in [10, 20]
  - bandwidth in [20, 30]

This is controlled by `SeededRandom`, so results are reproducible when a seed is fixed.

## References

- docs/s11227-021-03742-3.pdf
- docs/QESM.pdf
